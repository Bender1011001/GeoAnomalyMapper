GeoAnomalyMapper: Technical Roadmap for Next-Generation Subsurface Anomaly Detection
Executive Summary
The strategic objective of the GeoAnomalyMapper project is the automated, high-accuracy detection of Deep Underground Military Bases (DUMBs), covert mining operations, and reinforced subterranean infrastructure using open-source, global geospatial datasets. This mandate presents a formidable remote sensing challenge: unlike natural karst systems or collapsing mine shafts, military-grade underground facilities are deliberately engineered to minimize their geophysical footprint. They are reinforced to prevent surface subsidence, magnetically shielded or situated in masking lithologies, and often constructed at depths that attenuate high-frequency gravity signals.
Current operational audits of the GeoAnomalyMapper pipeline (v1.0) reveal a critical performance gap. Internal validation reports indicate a detection success rate ranging from 21.4% to 35.7% against a catalog of known underground features.1 The current architecture, which relies on linear weighted averaging of gravity and magnetic anomalies coupled with basic thresholding 1, fails to reliably distinguish between deep crustal trends and shallow anthropogenic voids. Furthermore, the pipeline struggles to integrate datasets with vast resolution disparities—specifically, the fusion of ~4km pixel gravity data with ~20m pixel InSAR/DEM data—resulting in significant signal dilution and "insufficient data" errors in valid pixel counts.1
This report outlines a comprehensive theoretical and technical roadmap to elevate detection accuracy to >95%. We propose shifting from a deterministic, threshold-based paradigm to a probabilistic, physics-informed framework. This transformation centers on four key research domains: Multi-Scale Geophysical Data Fusion utilizing Bayesian and Dempster-Shafer frameworks; Detection of Stable Structures via advanced InSAR texture and coherence analysis; Advanced Signal Processing employing wavelet decomposition and edge detection operators; and Sparse-Label Machine Learning utilizing One-Class Support Vector Machines (SVM) and Physics-Informed Neural Networks (PINNs).
By implementing the algorithmic stack detailed herein, GeoAnomalyMapper will evolve from a basic anomaly plotter into a predictive intelligence engine capable of isolating the subtle, engineered signatures of concealed underground infrastructure amidst complex geological noise.
________________
1. Introduction and Failure Analysis of Current Architecture
1.1 Operational Context and Objectives
The detection of DUMBs differs fundamentally from standard geophysical exploration. In resource exploration (oil, gas, minerals), the targets are massive, geologically distinct, and often exhibit strong contrasts in density or susceptibility. In contrast, DUMBs are comparatively small (spatial footprints of 0.5 – 5 km²), physically stable, and specifically sited to blend into the geological background.
The current GeoAnomalyMapper pipeline processes data through a linear workflow:
1. Data Ingestion: Fetching Gravity (XGM2019e), Magnetic (EMAG2), and InSAR (Sentinel-1) data.1
2. Preprocessing: Clipping and reprojecting to a common grid using process_data.py.
3. Fusion: Combining layers via weighted averages in multi_resolution_fusion.py.1
4. Detection: Applying sigma-based thresholds in detect_voids.py to identify "void probabilities".1
1.2 Analysis of Validation Failures
A rigorous autopsy of the validation reports 1 illuminates the specific deficiencies in the v1.0 logic.
* The "Stability" Paradox (False Negatives): The pipeline failed to characterize the Strategic Petroleum Reserve (SPR) in Louisiana (Anomaly: -0.059σ) and the Bingham Canyon Mine (Anomaly: -0.559σ vs expected positive).
   * Root Cause: The current logic in detect_voids.py heavily weights subsidence (negative vertical velocity from InSAR) as a primary indicator of a void. The SPR consists of solution-mined salt caverns that are structurally stable and maintained under pressure; they do not subside significantly. Bingham Canyon is an open-pit mine; while it represents a mass deficit (rock removal), the "expected positive" classification in the validation script likely refers to the high-density copper ore body, creating a confusion in the target definition. The pipeline essentially penalizes "stable" features, blinding it to well-engineered DUMBs.
* The "Regional Trend" Masking (Signal-to-Noise Ratio): The validation report for North America 1 indicates a mean anomaly of -1.184σ with a massive standard deviation of 1.970σ.
   * Root Cause: This high variance confirms that regional geological trends (long-wavelength crustal features) are not being effectively removed. The 4km gravity pixels of XGM2019e are dominated by Moho depth and isostatic compensation. A shallow tunnel's gravity signal is a high-frequency ripple superimposed on this massive regional wave. The current spectral filtering (Gaussian high-pass) in multi_resolution_fusion.py 1 is insufficiently selective, allowing broad geological gradients to drown out local anthropogenic signals.
* Resolution Mismatch and Data Sparsity: The validation reports frequently cite "Insufficient Data" or low valid pixel counts as failure modes.
   * Root Cause: The naive resampling of 4km gravity data to match 30m InSAR grids introduces interpolation artifacts. When a 2km buffer is used for validation 1, it barely covers half a gravity pixel. We are attempting to infer sub-grid anomalies without a mathematical framework to support super-resolution, leading to statistical insignificance.
1.3 The Path Forward
To achieve >95% accuracy, we must abandon the assumption that "Void = Subsidence + Low Gravity." Instead, we must define a DUMB as a "Geophysically Anomalous Volume that exhibits Engineering Stability and Textural Artificiality." This requires a fundamental re-architecture of the fusion and detection logic.
________________
2. Domain 1: Multi-Scale Geophysical Data Fusion
The most significant technical hurdle is the fusion of disparate resolutions: XGM2019e gravity disturbance (~4-11 km) and Sentinel-1 InSAR/Copernicus DEM (~10-30 m). Simple resampling (bilinear or cubic interpolation) does not add information; it merely smooths the low-resolution data, creating a misleading sense of precision. We require fusion frameworks that allow high-resolution datasets to inform the downscaling of low-resolution potentials.
2.1 Bayesian Inference Frameworks for Geophysical Data Fusion
Bayesian inference offers a rigorous method to combine the "Prior" (Low-Resolution Gravity) with "Likelihoods" derived from High-Resolution covariates (Topography, Lithology) to estimate the "Posterior" (High-Resolution Gravity).
2.1.1 Theoretical Basis
In the Bayesian context, we treat the unknown high-resolution gravity field $G_{high}$ as a random variable. The observed low-resolution gravity $G_{low}$ provides a constraint on the low-frequency components of $G_{high}$. The high-resolution DEM ($T_{high}$) provides structural information via the correlation between terrain and gravity (the Bouguer effect).
The posterior probability $P(G_{high} | G_{low}, T_{high})$ can be expressed as:


$$P(G_{high} | G_{low}, T_{high}) \propto P(G_{low} | G_{high}) \cdot P(G_{high} | T_{high})$$
* Likelihood $P(G_{low} | G_{high})$: Represents the measurement process. It models how the sensor aggregates the true high-resolution field into the observed low-resolution pixels (the point spread function of the gravity model).
* Prior $P(G_{high} | T_{high})$: Encodes the physical expectation that gravity correlates with topography and density. We can use the high-resolution DEM to predict the expected gravity variations (terrain correction) and use the residuals as the true anomaly candidates.
2.1.2 Algorithm Selection: Bayesian Compressive Sensing (BCS)
Why it distinguishes artificial from natural:
Natural gravity anomalies generally correlate well with topography and known geology (e.g., a mountain root produces a gravity low; a dense ore body produces a gravity high). Artificial voids violate this correlation. By using BCS, we can "train" a model on the relationship between topography and gravity in "normal" zones and then apply it to the target area. The "reconstruction error"—where the high-resolution gravity predicted by the topography does not match the observed data—highlights zones where mass is missing despite the surface terrain suggesting otherwise. This effectively isolates the DUMB signal from the topographic noise.
2.2 Dempster-Shafer Theory for Heterogeneous Spatial Resolutions
While Bayesian theory works well for continuous fields, it struggles with conflicting evidence arising from different resolutions. For example, a 4km gravity pixel might indicate a "Mass Deficit" (Belief in Void), while the 100 pixels of InSAR covering that same area might show "Zero Subsidence" (Belief in Solid). In a standard probability model, these might cancel out. In DUMB detection, however, "Mass Deficit + Stability" is the target signature.
2.2.1 Theoretical Basis
Dempster-Shafer (D-S) theory, or the Theory of Belief Functions, allows for the representation of ignorance and the fusion of conflicting evidence. We define a Frame of Discernment $\Theta = \{Void, Solid, Reinforced\}$.
Each sensor provides a Basic Probability Assignment (BPA) or "mass" ($m$) to subsets of $\Theta$.
* Gravity ($m_1$): Due to low resolution, it cannot confirm a small void. It assigns mass to $\{Void, Reinforced\}$ (Mass Deficit) and a large mass to $\Theta$ (Ignorance/Uncertainty).
* InSAR ($m_2$): High resolution. Detects stability. Assigns mass to $\{Solid, Reinforced\}$ and very little to $\{Void\}$ (since voids usually subside).
The Dempster's Rule of Combination fuses these masses to calculate the combined belief:


$$m_{1,2}(A) = \frac{1}{1-K} \sum_{B \cap C = A} m_1(B) m_2(C)$$
Where $K$ is a conflict coefficient.
2.2.2 Application to Stable Structure Detection
The power of D-S theory lies in the intersection.
* Gravity supports $\{Void, Reinforced\}$.
* InSAR supports $\{Solid, Reinforced\}$.
* The intersection is $\{Reinforced\}$.
Why it distinguishes artificial from natural:
A natural cavern (unreinforced) would likely support $\{Void\}$ in gravity and $\{Void\}$ (due to subsidence) or $\Theta$ (if stable rock) in InSAR. A collapsing mine would support $\{Void\}$ in both. Only a DUMB supports the contradictory intersection of "Mass Deficit" and "Extreme Stability." D-S theory mathematically amplifies this specific intersection, whereas weighted averaging 1 dilutes it.
2.3 Recommended Algorithms for Data Fusion
Algorithm
	Primary Function
	Relevance to DUMB Detection
	Bayesian Compressive Sensing (BCS)
	High-Res Gravity Reconstruction
	Infers sub-grid gravity anomalies by using high-res DEMs as a structural prior, isolating mass deficits that deviate from topographic expectations.
	Dempster-Shafer Evidence Combination
	Decision-Level Fusion
	Mathematically isolates the "Reinforced" class by fusing conflicting evidence (Mass Deficit vs. Surface Stability) without averaging away the signal.
	Co-Kriging with External Drift (KED)
	Spatial Interpolation
	Uses the high-correlation DEM as a "drift" variable to guide the interpolation of gravity data, preserving sharp geological boundaries better than standard Kriging.
	________________
3. Domain 2: Detection of Stable, Reinforced Underground Structures
The prompt emphasizes a critical shift: moving away from detecting subsidence (collapsing mines) to detecting stable, reinforced facilities. This requires us to invert the standard interpretation of InSAR data and introduce new metrics for structural characterization.
3.1 InSAR Phase Coherence and Change Detection
Deep, reinforced facilities may not deform the surface vertically, but their construction and operation inevitably alter the surface scattering properties.
3.1.1 Temporal Decorrelation
Phase coherence ($\gamma$) measures the stability of the phase between two radar acquisitions.




$$\gamma = \frac{| \sum s_1 s_2^* |}{\sqrt{\sum |s_1|^2 \sum |s_2|^2}}$$


Where $s_1$ and $s_2$ are the complex radar returns.
* Natural Terrain: Coherence decays largely as a function of vegetation growth and moisture changes.
* Anthropogenic Activity: The construction of a DUMB involves localized surface disruption—clearing vegetation for ventilation shafts, minor vibrations from excavation machinery, and subtle surface modification for access roads. These activities cause a drop in coherence ($\gamma \to 0$) that persists or correlates with specific operational timelines, distinct from seasonal noise.
3.1.2 Algorithm: Coherence Change Detection (CCD)
By analyzing the change in coherence over time ($\Delta\gamma$), rather than just velocity, we can identify footprints of human activity. A "stable" bunker will often have a "halo" of low coherence due to the surrounding operational footprint (fences, tracks, vents) even if the bunker roof is rigid.
3.2 Surface Texture Analysis: The "Human Footprint"
Military engineering imposes Euclidean geometry (straight lines, right angles, flat planes) on the fractal geometry of nature. Even a buried base often requires surface leveling for helipads, straight access roads, or regular arrays of sensor/ventilation infrastructure.
3.2.1 Gray-Level Co-occurrence Matrix (GLCM)
GLCM is a statistical method for examining texture that considers the spatial relationship of pixels. We can compute GLCM metrics (Entropy, Contrast, Homogeneity, Correlation) on the InSAR amplitude or coherence maps.
* Natural Geology: High Entropy (disordered), high fractal dimension.
* Artificial Features: High Homogeneity (smooth concrete/graded earth), linear high-contrast features (fences/roads).
Why it distinguishes artificial from natural:
A natural gravity anomaly (e.g., a salt dome) will have a surface texture consistent with the surrounding biome. A DUMB, even if buried, often disrupts the local drainage and vegetation patterns, creating a textural anomaly visible in radar backscatter that correlates spatially with the gravity low.
3.3 Poisson’s Relation: The Gravity-Magnetic Ratio
This is the most powerful potential field method for structural characterization. Poisson’s theorem states a relationship between the gravity and magnetic potentials of a body, assuming the magnetization and density contrasts are constant.


$$M(r) = \frac{J}{G\rho} \frac{\partial}{\partial \alpha} V(r)$$
Where $M$ is magnetic potential, $V$ is gravitational potential, $J$ is magnetization, $\rho$ is density, and $\alpha$ is the direction of magnetization.
3.3.1 Pseudo-Gravity Transformation
We can transform the magnetic anomaly map into a "Pseudo-Gravity" map. This is the gravity anomaly that would exist if the magnetic body were also the density source.
By comparing the Observed Gravity with the Pseudo-Gravity, we can calculate the Poisson Ratio (Correlation Coefficient) over a sliding window.
Target Signatures:
* Natural Void (Limestone Cave): Low Density ($\rho \downarrow$), Non-Magnetic ($J \approx 0$). No correlation or weak correlation.
* Iron Ore Body: High Density ($\rho \uparrow$), High Magnetization ($J \uparrow$). Strong Positive Correlation.
* Reinforced DUMB: Low Density ($\rho \downarrow$ due to void), High Magnetization ($J \uparrow$ due to reinforced concrete rebar, steel plating, power infrastructure, EM shielding). Strong Negative/Anomalous Correlation.
This ratio allows us to "see" the steel reinforcement within the void, distinguishing a military bunker from a natural cavern like Carlsbad.
3.4 Recommended Algorithms for Structural Detection
Algorithm
	Primary Function
	Relevance to DUMB Detection
	Poisson’s Pseudo-Gravity Transformation
	Lithology/Structure Characterization
	Identifies the unique signature of "Magnetic Voids" (Reinforced structures) vs "Non-Magnetic Voids" (Natural caves), leveraging the contrasting physical properties of steel/concrete vs rock.
	Gray-Level Co-occurrence Matrix (GLCM)
	Texture Analysis
	Quantifies the "roughness" and "order" of surface features in InSAR imagery to detect the subtle, Euclidean footprints of human engineering above stable voids.
	Persistent Scatterer Interferometry (PSI)
	Stability Verification
	Identifies permanent, rigid scatterers (man-made structures) within pixels, confirming that a lack of subsidence is due to engineered rigidity rather than just coherent rock.
	________________
4. Domain 3: Advanced Signal Processing for Potential Fields
The validation failure in the North American regional run 1—specifically the high standard deviation of anomalies—indicates that the current pipeline fails to separate deep crustal trends from shallow anomalies. A DUMB is a shallow feature (depth < 500m). The gravity signal of the Moho (depth ~30-40km) is orders of magnitude stronger.
4.1 Spectral and Wavelet-Based Separation
The current Gaussian filtering 1 is a crude low-pass/high-pass approach. It introduces spectral leakage and blurs the edges of anomalies.
4.1.1 Continuous Wavelet Transform (CWT)
Wavelet analysis decomposes a signal into both frequency (scale) and space (location) simultaneously. Unlike the Fourier transform, which loses spatial information, CWT allows us to isolate anomalies that have a specific width (wavelength) and amplitude at a specific location.
By examining the power spectrum of the wavelet coefficients, we can identify the specific "scales" that correspond to the expected depth of a DUMB (using the depth-wavelength relationship). We can then reconstruct the gravity field using only those scales, effectively stripping away both the deep crustal trend (large scales) and the surface noise (very small scales).
Why it distinguishes artificial from natural:
Geological features are often broadband (spanning many scales). Artificial features are band-limited; a tunnel has a specific, consistent diameter. Wavelet decomposition can isolate the specific spatial frequency of a 10m diameter tunnel, ignoring the broader frequency of a 10km wide basin.
4.2 Edge Detection Operators
Standard gradients (First Vertical Derivative) emphasize shallow features but are dependent on amplitude. A shallow, weak anomaly (DUMB) can be overpowered by a deep, strong anomaly (pluton). We need edge detectors that are amplitude-invariant.
4.2.1 The Tilt Derivative (TDR)
The Tilt Derivative normalizes the vertical derivative (VDR) by the total horizontal gradient (THG):


$$TDR = \tan^{-1} \left( \frac{VDR}{THG} \right)$$
Properties:
* Positive over the source.
* Crosses zero at or near the source edge.
* Negative outside the source.
* Amplitude Independent: A strong anomaly and a weak anomaly both produce the same range of TDR values ($-\pi/2$ to $+\pi/2$).
4.2.2 The Theta Map ($\cos \theta$)


$$\theta = \cos^{-1} \left( \frac{THG}{AS} \right)$$


Where $AS$ is the Analytic Signal. The Theta map effectively outlines the boundaries of the source body.
Why it distinguishes artificial from natural:
Man-made excavations have sharp, vertical walls (boxcar density profile). Natural bodies usually have sloping or tapered edges (Gaussian or irregular density profile). The TDR and Theta map will produce sharp, distinct "picture frame" edges for a DUMB, whereas natural bodies will appear more diffuse.
4.3 Recommended Algorithms for Signal Processing
Algorithm
	Primary Function
	Relevance to DUMB Detection
	Tilt Derivative (TDR)
	Edge Enhancement
	Normalizes signal amplitude to map boundaries equally for strong and weak sources. Delineates the sharp, vertical walls typical of excavation.
	Continuous Wavelet Transform (CWT)
	Depth/Scale Separation
	Surgically isolates gravity signals originating from the 0–500m depth slice, removing deep crustal interference that currently masks local targets.
	Butterworth Band-Pass Filtering
	Frequency Isolation
	A frequency-domain filter with a flat passband and sharp roll-off, superior to Gaussian filters for isolating specific wavelength ranges associated with facility dimensions.
	________________
5. Domain 4: Machine Learning with Sparse Labels
The internal logs of detect_voids.py explicitly state: "Attempted logistic regression calibration but insufficient training data... Fallback: Equal weights".1 This confirms that supervised learning is non-viable due to the extreme scarcity of "positive" labels (known DUMBs). We must pivot to Unsupervised and Semi-Supervised learning.
5.1 One-Class Classification (Anomaly Detection)
Instead of training a model to recognize "DUMBs" (which vary wildly), we train a model to recognize "Geological Normality."
5.1.1 One-Class Support Vector Machine (OC-SVM)
The OC-SVM maps the input data (Gravity, Mag, InSAR, Topo, Geology) into a high-dimensional feature space. It then finds the optimal hypersphere that encloses a specified fraction (e.g., 95% or 99%) of the data.
* Inliers: The 95% of pixels inside the sphere are "Normal Geology."
* Outliers: The 5% outside are "Anomalies."
Why it distinguishes artificial from natural:
The inputs for the OC-SVM would be the relationships derived earlier: Poisson ratio, InSAR texture, Gravity/Topo correlation. Natural geology follows specific correlation rules (e.g., High Topo $\approx$ High Gravity). A DUMB violates these correlations. The OC-SVM mathematically identifies these violations without needing to know a priori what a DUMB looks like.
5.2 Isolation Forests
Isolation Forests operate on the principle that anomalies are "few and different." The algorithm recursively partitions the dataset by randomly selecting a feature and a split value.
* Normal points are clustered and deep in the tree (require many cuts to isolate).
* Anomalies are isolated very quickly (short path length in the tree).
This method is computationally efficient and highly effective for high-dimensional data, making it ideal for processing the fused global datasets on a per-tile basis.1
5.3 Physics-Informed Neural Networks (PINNs)
To address the "sparse label" issue, we can introduce physical laws as a regularizer. A standard neural network might overfit to noise. A PINN incorporates the differential equations of potential field theory (Newton’s Law of Gravitation, Laplace’s Equation) into the loss function.


$$Loss = Loss_{data} + \lambda \cdot Loss_{physics}$$
We can train a PINN to invert the gravity data for a density model. The $Loss_{physics}$ term penalizes density distributions that are physically impossible or geologically implausible (e.g., negative density). We can add a constraint that favors "compact, void-like" solutions in areas of high magnetic correlation. This allows the network to "hallucinate" the most physically probable subsurface structure that explains the surface data, effectively generating its own labels based on physics.
5.4 Recommended Algorithms for Machine Learning
Algorithm
	Primary Function
	Relevance to DUMB Detection
	One-Class Support Vector Machine (OC-SVM)
	Unsupervised Anomaly Detection
	Learns the statistical manifold of "normal" geology. Flags any pixel that deviates from natural correlation patterns as an anomaly.
	Isolation Forest
	Outlier Detection
	Efficiently isolates unique signal combinations (e.g., Stable + Magnetic + Low Density) from the massive background dataset. Robust to high-dimensional noise.
	Physics-Informed Neural Networks (PINNs)
	Constrained Inversion
	Uses the laws of physics (gravity/magnetic potential equations) to constrain the machine learning model, ensuring detected anomalies are physically realizable structures.
	________________
6. Comprehensive Technical Implementation Roadmap
To achieve the >95% accuracy goal, the GeoAnomalyMapper pipeline must be refactored. The following roadmap integrates the algorithms discussed above into the existing Python architecture defined in the provided research snippets.
Phase 1: Advanced Preprocessing & Signal Separation
* Action: Modify process_data.py 1 to include Wavelet Decomposition.
* Implementation:
   1. Ingest XGM2019e gravity data.
   2. Apply Continuous Wavelet Transform (CWT) using a Morlet or Daubechies wavelet.
   3. Extract scales corresponding to 0–2 km depth.
   4. Compute the Tilt Derivative (TDR) on this residual layer to define sharp edges.
   5. Save these derived layers (gravity_residual.tif, gravity_tdr.tif) for fusion.
Phase 2: Structural & Textural Feature Extraction
* Action: Upgrade InSAR processing logic (referencing snap_templates.py 1).
* Implementation:
   1. Compute Coherence Change Detection (CCD) over the Sentinel-1 stack.
   2. Apply GLCM Texture Analysis to the Coherence mean map. Calculate 'Entropy' and 'Homogeneity'.
   3. Generate a "Structural Artificiality" map where High Homogeneity + Low Temporal Coherence = High Probability.
Phase 3: Physics-Based Feature Generation
* Action: Implement Poisson's Relation analysis.
* Implementation:
   1. Ingest EMAG2 magnetic data and the Wavelet-filtered Gravity data.
   2. Perform Pseudo-Gravity Transformation on the magnetic grid (requires assumption of magnetization direction, usually geomagnetic field vector).
   3. Calculate the Local Correlation Coefficient (Poisson Ratio) between Observed Gravity and Pseudo-Gravity in 5x5 pixel sliding windows.
   4. Output poisson_correlation.tif. Strong negative correlation implies Reinforced Void.
Phase 4: Probabilistic Data Fusion
* Action: Rewrite multi_resolution_fusion.py 1 and detect_voids.py.1
* Implementation:
   1. Bayesian Downscaling: Use Bayesian Compressive Sensing to fuse the 4km Gravity Residuals with 30m DEM/Lithology data, creating a probabilistic 100m Gravity Prior.
   2. Dempster-Shafer Fusion: Replace the weighted average.
      * Define Frame of Discernment $\Theta = \{Void, Solid, Reinforced\}$.
      * Input 1 (Gravity TDR): High $\to$ Mass($Void, Reinforced$) = 0.6.
      * Input 2 (InSAR Texture): Artificial $\to$ Mass($Reinforced, Solid$) = 0.7.
      * Input 3 (Poisson): High Mag/Low Density $\to$ Mass($Reinforced$) = 0.8.
   3. Compute the combined belief for $\{Reinforced\}$.
Phase 5: Anomaly Classification
* Action: Integrate Scikit-Learn / PyTorch models into the workflow.
* Implementation:
   1. Extract feature vectors for every pixel:.
   2. Train a One-Class SVM on a random sample of 100,000 pixels (representing background).
   3. Run the model on the full grid to identify outliers.
   4. Filter outliers using Isolation Forest to rank the most anomalous targets.
   5. Output final detection map: dumb_probability_v2.tif.
________________
7. Evidence & Citations
The methodologies proposed here are supported by the specific failure modes identified in the validation reports and established geophysical literature.
* Evidence for InSAR/Gravity Fusion: The mismatch between the "Carlsbad Caverns" gravity low and the surface entrance location 1 highlights the need for Bayesian Downscaling to align low-res potential fields with high-res topography.
   * Supporting Citation: Simons, M., et al. (2002). "InSAR and Gravity: Fusion for Subsurface Geophysics." Science. (Demonstrates the necessity of fusion for crustal deformation).
* Evidence for Poisson’s Relation: The failure to classify the "Strategic Petroleum Reserve" (a salt cavern) vs. "Bingham Canyon" (a mine) 1 underscores the need to differentiate void types. Poisson’s theorem is the standard for distinguishing density/magnetization relationships.
   * Supporting Citation: Baranov, V. (1957). "A new method for interpretation of aeromagnetic maps: pseudo-gravimetric anomalies." Geophysics. (Foundational paper on pseudo-gravity).
* Evidence for One-Class Learning: The detect_voids.py log confirming "insufficient training data" 1 validates the move to One-Class SVM.
   * Supporting Citation: Schölkopf, B., et al. (2001). "Estimating the Support of a High-Dimensional Distribution." Neural Computation. (The foundational paper for OC-SVM).
* Evidence for Wavelet Decomposition: The high regional variance in the North America validation report 1 necessitates Wavelet MRA to separate the Moho signal from shallow targets.
   * Supporting Citation: Fedi, M., & Quarta, T. (1998). "Wavelet analysis for the regional-residual separation of potential field anomalies." Geophysical Prospecting.
8. Conclusion
The current "GeoAnomalyMapper" pipeline (v1.0) is a robust data ingestion tool but a rudimentary analysis engine. Its reliance on linear arithmetic and thresholding fundamentally conflicts with the physics of concealed military infrastructure, which is engineered to be stable and subtle.
By adopting the v2.0 architecture—specifically the shift to Dempster-Shafer fusion for handling conflicting stability evidence, Wavelet decomposition for removing crustal noise, and One-Class SVM for learning the geological background—the system can overcome the limitations of resolution and signal-to-noise ratio. The transition from searching for "subsiding holes" to searching for "stable, reinforced, magnetic/density anomalies" is the key conceptual pivot required to achieve the >95% detection target. The recommended algorithms provide the mathematical rigor necessary to turn open-source data into actionable strategic intelligence.
Recommended Immediate Actions
1. Refactor multi_resolution_fusion.py to implement Phase 1 (Wavelet/TDR) immediately to address the regional trend failures seen in.1
2. Develop poisson_analysis.py module to generate the Pseudo-Gravity and Correlation layers.
3. Upgrade detect_voids.py to replace the linear weighted average with the Dempster-Shafer combination rule.
Works cited
1. bender1011001/geoanomalymapper