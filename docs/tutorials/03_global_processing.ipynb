{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Global Processing with GeoAnomalyMapper (GAM)\n",
    "\n",
    "This advanced tutorial covers large-scale analysis, such as global anomaly mapping. We'll set up global processing, optimize for parallelism, handle large datasets, and manage memory.\n",
    "\n",
    "**Objectives**:\n",
    "- Configure for global tiling.\n",
    "- Use Dask for parallel execution.\n",
    "- Optimize performance and memory.\n",
    "- Process and merge global results.\n",
    "\n",
    "**Prerequisites**: Complete 01/02; high-RAM machine (32GB+) or cloud recommended. Global run may take hours/days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import yaml\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from gam.core.pipeline import run_pipeline\n",
    "from gam.core.global_processor import GlobalProcessor  # For tiling\n",
    "import gam\n",
    "print(f\"GAM version: {gam.__version__}\")\n",
    "print(f\"Dask version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Global Configuration Setup\n",
    "\n",
    "Define global bbox (entire Earth) and tiling parameters. Use coarse resolution for feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global bbox: full Earth\n",
    "global_bbox = (-90, 90, -180, 180)\n",
    "modalities = [\"gravity\"]  # Start with one for demo; add others for full\n",
    "\n",
    "# Global config: Coarse grid, large tiles\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"bbox\": global_bbox,\n",
    "        \"modalities\": modalities,\n",
    "        \"cache_dir\": \"../data/global_cache\"  # Large cache needed\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"grid_res\": 1.0  # Coarse (1° ~111km) for global\n",
    "    },\n",
    "    \"modeling\": {\n",
    "        \"threshold\": 3.0,  # Strict for global noise\n",
    "        \"max_iterations\": 30  # Faster convergence\n",
    "    },\n",
    "    \"visualization\": {\n",
    "        \"map_type\": \"2d\",\n",
    "        \"export_formats\": [\"geotiff\", \"csv\"]\n",
    "    },\n",
    "    \"core\": {\n",
    "        \"output_dir\": \"../results/global\",\n",
    "        \"parallel_workers\": -1,  # All cores\n",
    "        \"tile_size\": 30,  # 30° tiles (12 tiles lat x 12 lon = 144 total)\n",
    "        \"rate_limit_delay\": 2.0  # Slower for global API calls\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"global_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "print(\"Global config saved. Expected tiles: ~144\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Parallel Processing with Dask\n",
    "\n",
    "Launch Dask client for distributed computing. Monitor via dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask client (local cluster)\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')  # Adjust to your machine\n",
    "print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "# For cloud/HPC: Use dask-jobqueue or Kubernetes\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# cluster = SLURMCluster(...)\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Global Processing Setup and Execution\n",
    "\n",
    "Use GlobalProcessor to tile and parallelize. This decomposes Earth into tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global processor\n",
    "processor = GlobalProcessor(config)\n",
    "\n",
    "# Generate tiles (30°)\n",
    "tiles = processor.generate_tiles(global_bbox, tile_size=config['core']['tile_size'])\n",
    "print(f\"Generated {len(tiles)} tiles\")\n",
    "\n",
    "# Parallel run on tiles (delayed for Dask)\n",
    "from dask import delayed\n",
    "delayed_runs = [delayed(run_pipeline)(\n",
    "    bbox=tile,\n",
    "    modalities=modalities,\n",
    "    config=config,\n",
    "    output_dir=f\"{config['core']['output_dir']}/tile_{i}\"\n",
    ") for i, tile in enumerate(tiles)]\n",
    "\n",
    "# Compute (parallel execution)\n",
    "print(\"Starting global computation... Monitor dashboard.\")\n",
    "tile_results = client.compute(delayed_runs).result()\n",
    "\n",
    "print(\"Global processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merge Global Results\n",
    "\n",
    "Combine tile anomalies, handle overlaps, generate global map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge anomalies from tiles\n",
    "all_anomalies = []\n",
    "for res in tile_results:\n",
    "    all_anomalies.append(res['anomalies'])\n",
    "global_anomalies = pd.concat(all_anomalies, ignore_index=True)\n",
    "\n",
    "# Handle overlaps (simple: average confidence)\n",
    "if len(global_anomalies) > 0:\n",
    "    global_anomalies = global_anomalies.groupby(['lat', 'lon', 'depth']).agg({\n",
    "        'confidence': 'mean',\n",
    "        'score': 'max',\n",
    "        'anomaly_type': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "print(f\"Global anomalies: {len(global_anomalies)}\")\n",
    "global_anomalies.to_csv(config['core']['output_dir'] + \"/global_anomalies.csv\", index=False)\n",
    "\n",
    "# Global visualization (coarse map)\n",
    "global_viz = generate_visualization(\n",
    "    global_anomalies,\n",
    "    type=\"2d\",\n",
    "    output_dir=config['core']['output_dir'],\n",
    "    config=config['visualization']\n",
    ")\n",
    "print(\"Global map saved:\", global_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Performance Optimization and Memory Management\n",
    "\n",
    "Tips for scaling: Chunking, spilling, monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring example (using psutil)\n",
    "import psutil\n",
    "print(f\"Current RAM usage: {psutil.virtual_memory().percent:.1f}%\")\n",
    "\n",
    "# Optimization tips in code:\n",
    "# 1. Lazy loading: Use dask.array for large grids\n",
    "# Example: processed = xr.open_dataset('large_grid.nc', chunks={'lat': 1000})\n",
    "\n",
    "# 2. Reduce resolution for scouting\n",
    "scout_config = config.copy()\n",
    "scout_config['preprocessing']['grid_res'] = 5.0  # Very coarse\n",
    "\n",
    "# 3. Spilling: Set in Dask client (already 4GB limit above)\n",
    "\n",
    "# Re-run a single tile with optimization\n",
    "sample_tile = tiles[0]\n",
    "optimized_results = run_pipeline(\n",
    "    bbox=sample_tile,\n",
    "    modalities=modalities,\n",
    "    config=scout_config\n",
    ")\n",
    "print(\"Optimized tile complete (coarse res).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Large Datasets\n",
    "\n",
    "- **Storage**: Use external SSD/cloud for cache (100GB+ for global).\n",
    "- **Batching**: Process modalities separately if memory tight.\n",
    "- **Cloud Scaling**: Deploy Dask on AWS EMR or Google Dataproc.\n",
    "- **Monitoring**: Watch dashboard for task failures; resume from checkpoints.\n",
    "\n",
    "**Example Output**: Global CSV with worldwide anomalies; GeoTIFF for world map.\n",
    "\n",
    "For production, integrate with HPC schedulers. See [Deployment Guide](../configuration/deployment.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Global processing enables worldwide anomaly mapping. Optimize based on hardware; start with subsets.\n",
    "\n",
    "Next: Explore use cases in [Examples](../examples/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}