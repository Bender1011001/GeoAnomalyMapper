import os
import time
import logging
import numpy as np
import rasterio
from rasterio.enums import Resampling
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import matplotlib.pyplot as plt

from utils.config import load_config

# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# 1. Configuration & Constants
# ==========================================
# Default configuration if config.yaml is missing or incomplete
DEFAULT_CONFIG = {
    "GPU_ID": 0,
    "SEED": 42,
    "LR": 1e-3,
    "EPOCHS": 1000,
    "BATCH_SIZE": 1,
    "TILE_SIZE": 1024,
    "DEPTH_ESTIMATE": 200,   # Mean depth of sources (meters)
    "THICKNESS_ESTIMATE": 1000, # Thickness of the source layer (meters)
    "MAX_DENSITY": 800.0,    # Max density contrast (kg/m^3) - Increased for sensitivity
    "PHYSICS_WEIGHT": 10.0,  # Increased to prioritize data fit
    "SPARSITY_WEIGHT": 0.001, # Reduced to allow weaker signals
    "TV_WEIGHT": 0.01,       # Reduced to prevent over-smoothing
    "GRAD_WEIGHT": 0.5,      # Gradient/Edge preservation weight
    "USE_AMP": True,
    "TARGET_MODE": "mineral" # 'void', 'mineral', 'general'
}

def get_device(config):
    """
    Robustly determine the computation device.
    """
    if os.environ.get("FORCE_CPU_INVERSION", "0") == "1":
        logger.info("Forcing CPU execution based on environment variable.")
        return torch.device("cpu"), False
    
    if torch.cuda.is_available():
        device_id = config.get('GPU_ID', 0)
        device = torch.device(f"cuda:{device_id}")
        logger.info(f"Using GPU: {torch.cuda.get_device_name(device_id)}")
        return device, config.get('USE_AMP', True)
    else:
        logger.warning("CUDA not available. Falling back to CPU.")
        return torch.device("cpu"), False

# ==========================================
# 2. Physics Layer (Computational Geophysics)
# ==========================================
class GravityPhysicsLayer(nn.Module):
    """
    Differentiable Forward Gravity Modeling Layer.

    Implements Parker's Oldenburg formula (flat-earth approximation) in the frequency domain.
    Calculates the vertical component of the gravitational field generated by a density distribution.

    NOTE: This is a "Physics-Guided" forward layer used to constrain the neural network output,
    ensuring it produces physically plausible density models. It is distinct from a
    pure "Collocation Point" PINN solver for PDEs, as it uses a spectral forward operator interaction.

    Attributes:
        pixel_size (float): Spatial resolution of the grid in meters.
        depth (float): Mean depth of the source layer in meters.
        thickness (float): Thickness of the source slab in meters.
        G (float): Gravitational constant.
    """
    def __init__(self, pixel_size_meters, mean_depth, thickness=1000.0):
        super().__init__()
        self.pixel_size = pixel_size_meters
        self.depth = mean_depth
        self.thickness = thickness
        self.G = 6.674e-11
        self.SI_to_mGal = 1e5
        self.earth_filter = None

    def forward(self, density_map):
        """
        Args:
            density_map (Tensor): [B, C, H, W] Density contrast in kg/m^3.
            
        Returns:
            Tensor: [B, C, H, W] Simulated gravity anomaly in mGal.
        """
        B, C, H, W = density_map.shape
        
        # Padding to mitigate edge artifacts (spectral leakage)
        # We use reflection padding to maintain continuity at boundaries
        # For CuFFT in FP16 (AMP), dimensions often perform best or require powers of 2 
        # (or at least multiples of 8/16/32, strictly powers of 2 for some implementations).
        # Let's target next power of 2 or at least a multiple of 32 for safety and performance.
        
        target_h = 1 << (H + int(H * 0.25) - 1).bit_length()
        target_w = 1 << (W + int(W * 0.25) - 1).bit_length()
        
        # Ensure minimal padding of ~25%
        if target_h < H * 1.25: target_h *= 2
        if target_w < W * 1.25: target_w *= 2
        
        pad_total_h = target_h - H
        pad_total_w = target_w - W
        
        pad_h_top = pad_total_h // 2
        pad_h_bottom = pad_total_h - pad_h_top
        pad_w_left = pad_total_w // 2
        pad_w_right = pad_total_w - pad_w_left

        density_padded = F.pad(density_map, (pad_w_left, pad_w_right, pad_h_top, pad_h_bottom), mode='reflect')
        B, C, H_pad, W_pad = density_padded.shape

        # 1. Precompute Filter (Lazy Loading)
        if (self.earth_filter is None) or (self.earth_filter.shape[-2:] != (H_pad, W_pad)):
            device = density_map.device
            
            # Spatial frequencies (cycles/meter)
            freq_y = torch.fft.fftfreq(H_pad, d=self.pixel_size).to(device)
            freq_x = torch.fft.fftfreq(W_pad, d=self.pixel_size).to(device)
            
            KY, KX = torch.meshgrid(freq_y, freq_x, indexing='ij')
            
            # Wavenumber k = 2 * pi * f
            K_magnitude = torch.sqrt(KX**2 + KY**2)
            k_angular = 2 * np.pi * K_magnitude
            
            # Handle DC component (k=0) to avoid singularity if needed, 
            # though exp(-0) is 1, so it's safe.
            
            # Parker's Formula (First Term approximation for a slab):
            # F[g] = 2 * pi * G * exp(-|k| * z0) * F[rho] * thickness
            # We multiply by thickness to convert volume density (kg/m^3) to surface density (kg/m^2)
            
            filter_response = (2 * np.pi * self.G * torch.exp(-k_angular * self.depth) * self.thickness)
            self.earth_filter = filter_response.unsqueeze(0).unsqueeze(0)

        # 2. FFT Convolution
        d_fft = torch.fft.fft2(density_padded)
        g_fft = d_fft * self.earth_filter
        gravity_pred_padded = torch.real(torch.fft.ifft2(g_fft))
        
        # 3. Crop back to original size
        # 3. Crop back to original size
        gravity_pred = gravity_pred_padded[:, :, pad_h_top:pad_h_top+H, pad_w_left:pad_w_left+W]
        
        return gravity_pred * self.SI_to_mGal

# ==========================================
# 3. Neural Network Architecture (ML Engineer)
# ==========================================
class DoubleConv(nn.Module):
    """(Conv -> InstanceNorm -> LeakyReLU) * 2"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.InstanceNorm2d(out_channels, affine=True),
            nn.LeakyReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.InstanceNorm2d(out_channels, affine=True),
            nn.LeakyReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

class DensityUNet(nn.Module):
    """
    U-Net architecture optimized for density inversion.
    """
    def __init__(self, max_density=500.0):
        super().__init__()
        self.max_density = max_density
        
        self.inc = DoubleConv(1, 32)
        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(32, 64))
        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))
        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))
        
        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv1 = DoubleConv(256 + 128, 128)
        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv2 = DoubleConv(128 + 64, 64)
        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv3 = DoubleConv(64 + 32, 32)
        
        self.outc = nn.Conv2d(32, 1, kernel_size=1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        
        x = self.up1(x4)
        if x.shape != x3.shape: x = F.interpolate(x, size=x3.shape[2:])
        x = torch.cat([x, x3], dim=1)
        x = self.conv1(x)
        
        x = self.up2(x)
        if x.shape != x2.shape: x = F.interpolate(x, size=x2.shape[2:])
        x = torch.cat([x, x2], dim=1)
        x = self.conv2(x)
        
        x = self.up3(x)
        if x.shape != x1.shape: x = F.interpolate(x, size=x1.shape[2:])
        x = torch.cat([x, x1], dim=1)
        x = self.conv3(x)
        
        # Tanh activation to bound the output density
        return torch.tanh(self.outc(x)) * self.max_density

# ==========================================
# 4. Advanced Loss Functions
# ==========================================
class GradientLoss(nn.Module):
    """
    Penalizes lack of sharpness in edges using Sobel filters.
    Encourages geological realism (sharp boundaries).
    """
    def __init__(self):
        super().__init__()
        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        self.register_buffer('kernel_x', kernel_x)
        self.register_buffer('kernel_y', kernel_y)

    def forward(self, x):
        grad_x = F.conv2d(x, self.kernel_x, padding=1)
        grad_y = F.conv2d(x, self.kernel_y, padding=1)
        gradient_magnitude = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)
        # We want to maximize gradient at edges, but here we might want to match 
        # the gradient distribution of the prior? 
        # Actually, for inversion, we often want Total Variation (minimize gradient) to reduce noise,
        # OR we want to encourage sparsity of gradients (blocky models).
        # Here, we return the mean gradient magnitude to be minimized (TV-like) or maximized?
        # Standard TV minimizes this. Let's stick to standard TV in the main loop 
        # and use this class if we want to enforce specific structural similarity.
        return torch.mean(gradient_magnitude)

# ==========================================
# 5. Inversion Workflow
# ==========================================

# ==========================================
# 5. Inversion Workflow
# ==========================================
def invert_gravity(tif_path, output_path, lithology_path=None, target_mode=None, magnetic_guide_path=None):
    # Load Config
    try:
        app_config = load_config()
        # Merge with defaults for missing keys
        config = {**DEFAULT_CONFIG, **app_config.get('gravity_inversion', {})}
    except Exception:
        logger.warning("Could not load config.yaml, using defaults.")
        config = DEFAULT_CONFIG

    # Override mode
    mode = target_mode if target_mode else config.get('TARGET_MODE', 'mineral')
    logger.info(f"Inversion Mode: {mode.upper()}")

    # Setup Device
    device, use_amp = get_device(config)

    # --- Load Data ---
    with rasterio.open(tif_path) as src:
        data = src.read(1)
        profile = src.profile
        height, width = data.shape
        
        # Calculate pixel size
        # If CRS is geographic (lat/lon), approximate meters
        if src.crs.is_geographic:
            center_lat = (src.bounds.top + src.bounds.bottom) / 2
            deg_to_meter = 111320 * np.cos(np.deg2rad(center_lat))
            pixel_size = src.transform[0] * deg_to_meter
        else:
            pixel_size = src.transform[0] # Assumes projected CRS (meters)
            
        logger.info(f"Loaded {tif_path}: {data.shape}, Pixel Size: {pixel_size:.1f}m")

    # --- Load Lithology Prior ---
    prior_contrast = None
    if lithology_path and os.path.exists(lithology_path):
        try:
            with rasterio.open(lithology_path) as src:
                prior_data = src.read(
                    1,
                    out_shape=(height, width),
                    resampling=Resampling.nearest
                )
                # Convert absolute density to contrast (relative to standard crust 2670 kg/m3)
                prior_contrast_np = prior_data - 2670.0
                prior_contrast = torch.from_numpy(prior_contrast_np).float().unsqueeze(0).unsqueeze(0).to(device)
                logger.info(f"Loaded Lithology Prior from {lithology_path}")
        except Exception as e:
            logger.warning(f"Failed to load lithology prior: {e}")

    # --- Load Magnetic Structural Guide ---
    guide_weights = None
    if magnetic_guide_path and os.path.exists(magnetic_guide_path):
        try:
            from loss_functions import calculate_weights_from_magnetic_gradient
            with rasterio.open(magnetic_guide_path) as src:
                mag_data = src.read(
                    1,
                    out_shape=(height, width),
                    resampling=Resampling.bilinear
                )
                mag_tensor = torch.from_numpy(mag_data).float().to(device)
                # Calculate weights: High at smooth areas, Low at edges
                # Using somewhat sensitive beta for strong edge preservation
                guide_weights = calculate_weights_from_magnetic_gradient(mag_tensor, beta=1.5).unsqueeze(0).unsqueeze(0)
                logger.info(f"Loaded Magnetic Guide from {magnetic_guide_path}")
        except Exception as e:
            logger.warning(f"Failed to load magnetic guide: {e}")

    # --- Preprocessing ---
    # Z-score normalization for stability
    grav_mean = np.nanmean(data)
    grav_std = np.nanstd(data)
    if grav_std == 0: grav_std = 1
    data_norm = (data - grav_mean) / grav_std
    data_norm = np.nan_to_num(data_norm, nan=0.0)

    inp_tensor = torch.from_numpy(data_norm).float().unsqueeze(0).unsqueeze(0).to(device)
    target_gravity = torch.from_numpy(data).float().unsqueeze(0).unsqueeze(0).to(device)
    
    target_mask = ~torch.isnan(target_gravity)
    target_gravity = torch.nan_to_num(target_gravity, nan=0.0)

    # --- Initialize Model & Optimizer ---
    model = DensityUNet(max_density=config['MAX_DENSITY']).to(device)
    physics = GravityPhysicsLayer(
        pixel_size,
        config['DEPTH_ESTIMATE'],
        thickness=config.get('THICKNESS_ESTIMATE', 1000.0)
    ).to(device)
    
    # Initialize Losses
    from loss_functions import StructureGuidedTVLoss
    sg_tv_loss_fn = StructureGuidedTVLoss().to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config['LR'])
    scheduler = CosineAnnealingLR(optimizer, T_max=config['EPOCHS'], eta_min=1e-5)
    scaler = GradScaler('cuda', enabled=use_amp)
    
    # --- Training Loop ---
    logger.info("Starting Inversion...")
    training_start = time.time()
    
    model.train()
    loop = tqdm(range(config['EPOCHS']), desc="Inverting")
    
    for epoch in loop:
        optimizer.zero_grad()
        
        device_type = 'cuda' if 'cuda' in str(device) else 'cpu'
        
        with autocast(device_type, enabled=use_amp):
            # 1. Predict Residual Density
            pred_residual = model(inp_tensor)
            
            # 2. Combine with Prior (if exists)
            if prior_contrast is not None:
                total_contrast = prior_contrast + pred_residual
            else:
                total_contrast = pred_residual
                
            # 3. Physics Simulation
            sim_gravity = physics(total_contrast)
            
            # 4. Loss Calculation
            # A. Data Fidelity (MSE)
            loss_data = F.mse_loss(sim_gravity[target_mask], target_gravity[target_mask])
            
            # B. Sparsity (L1) - Encourage simple models
            loss_sparsity = torch.mean(torch.abs(pred_residual))
            
            # C. Total Variation (TV) or Structure-Guided TV
            if guide_weights is not None:
                # Use the new structure-guided loss
                # Normalize by number of elements to keep scale comparable to mean() used in standard TV
                loss_tv = sg_tv_loss_fn(pred_residual, guide_weights) / pred_residual.numel()
            else:
                # Standard TV
                diff_i = torch.abs(pred_residual[:, :, 1:, :] - pred_residual[:, :, :-1, :])
                diff_j = torch.abs(pred_residual[:, :, :, 1:] - pred_residual[:, :, :, :-1])
                loss_tv = torch.mean(diff_i) + torch.mean(diff_j)
            
            # D. Mode-Specific Bias
            loss_bias = 0.0
            if mode == 'void':
                # Penalize positive mass (we want negative anomalies)
                loss_bias = torch.mean(F.relu(pred_residual))
            elif mode == 'mineral':
                # Penalize negative mass (we want positive anomalies)
                loss_bias = torch.mean(F.relu(-pred_residual))

            # Weighted Sum
            loss = (config['PHYSICS_WEIGHT'] * loss_data) + \
                   (config['SPARSITY_WEIGHT'] * loss_sparsity) + \
                   (config['TV_WEIGHT'] * loss_tv) + \
                   (0.1 * loss_bias)

        # 5. Backpropagation
        if use_amp:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()
            
        scheduler.step()
        
        if epoch % 10 == 0:
            loop.set_postfix(loss=loss.item(), mse=loss_data.item(), lr=optimizer.param_groups[0]['lr'])

    training_time = time.time() - training_start
    logger.info(f"Training completed in {training_time:.1f}s")

    # --- Save Result ---
    save_start = time.time()
    model.eval()
    with torch.no_grad():
        # We want the TOTAL density contrast map
        pred_residual = model(inp_tensor)
        if prior_contrast is not None:
            final_density = (prior_contrast + pred_residual).squeeze().cpu().numpy()
        else:
            final_density = pred_residual.squeeze().cpu().numpy()

    profile.update(dtype=rasterio.float32, count=1, compress='deflate')
    with rasterio.open(output_path, 'w', **profile) as dst:
        dst.write(final_density.astype(np.float32), 1)
        dst.set_band_description(1, "Density Contrast (kg/m3)")

    save_time = time.time() - save_start
    logger.info(f"Inversion Complete. Saved to {output_path}")

# ==========================================
# 6. Execution Entry Point
# ==========================================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Physics-Informed Gravity Inversion")
    parser.add_argument("--input", default="data/processed/gravity_residual.tif", help="Input Gravity Residual GeoTIFF")
    parser.add_argument("--output", default="data/outputs/density_contrast_map.tif", help="Output Density Contrast GeoTIFF")
    parser.add_argument("--lithology", default=None, help="Optional Lithology Prior GeoTIFF")
    parser.add_argument("--magnetic_guide", default=None, help="Optional Magnetic/Structural Guide GeoTIFF for regularization")
    parser.add_argument("--mode", default=None, choices=['void', 'mineral', 'general'], help="Inversion Target Mode")
    
    args = parser.parse_args()
    
    if os.path.exists(args.input):
        # Ensure output directory exists
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        
        invert_gravity(args.input, args.output, args.lithology, args.mode, args.magnetic_guide)
    else:
        logger.error(f"Input file '{args.input}' not found.")