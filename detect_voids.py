
#!/usr/bin/env python3
"""
Advanced Void Detection - Multi-Layer Probability Mapping

Combines multiple geophysical datasets to identify probable underground voids:
- Gravity anomalies (density deficits)
- InSAR subsidence (surface deformation)
- Lithology (karst-prone rock types)
- Seismic velocity (low-velocity zones)

Optimized for detecting voids at 20-300 feet (6-100m) depth.

Usage:
    python detect_voids.py --region "lat_min,lat_max,lon_min,lon_max" --output void_map.tif
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, Optional, Tuple
import numpy as np
import rasterio
from rasterio.transform import from_bounds
from rasterio.warp import reproject, Resampling
from rasterio.merge import merge
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

from project_paths import DATA_DIR, OUTPUTS_DIR

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION
# ============================================================================

OUTPUT_DIR = OUTPUTS_DIR / "void_detection"

# Data paths - using processed data
GRAVITY_PATH = DATA_DIR / "processed" / "gravity" / "gravity_processed.tif"
MAGNETIC_PATH = DATA_DIR / "processed" / "magnetic" / "magnetic_processed.tif"
INSAR_PATH = DATA_DIR / "processed" / "insar" / "insar_processed.tif"
LITHOLOGY_PATH = DATA_DIR / "processed" / "lithology"  # Directory for lithology
DEM_PATH = DATA_DIR / "processed" / "dem" / "dem_processed.tif"

# Void detection thresholds
THRESHOLDS = {
    'gravity_negative': -1.0,      # σ units (negative anomaly = density deficit)
    'subsidence_rate': -5.0,       # mm/year (negative = sinking)
    'karst_lithology': [            # Karst-prone rock types
        'limestone', 'dolomite', 'marble', 'carbonate',
        'evaporite', 'gypsum', 'anhydrite', 'halite'
    ],
    'seismic_low_velocity': 3.0,   # km/s (low velocity = voids/fractures)
}

# Detection threshold from Phase 3 sensitivity analysis
DETECTION_THRESHOLD_SIGMA = 0.35  # Calibrated from threshold sensitivity analysis (Phase 3)

# Weighting for probability calculation (default/fallback).
# Prefer calibrated weights if available on disk.
DEFAULT_WEIGHTS = {
    'gravity': 0.35,      # Gravity anomalies
    'magnetic': 0.25,     # Magnetic anomalies (separate from gravity)
    'insar': 0.35,        # InSAR subsidence
    'lithology': 0.20,    # Rock type susceptibility
    'seismic': 0.10,      # Seismic velocity
}

# CALIBRATION METADATA (from scientific action plan execution):
# - Attempted logistic regression calibration but insufficient training data
# - Ground-truth features had poor overlap with raster coverage
# - Fallback: Equal weights for available layers (gravity, magnetic, lithology)
# - Threshold: 0.35σ from sensitivity analysis (weak signal, use with caution)
# - Date: 2025-11-13
# - Note: These are NOT scientifically-calibrated weights due to data limitations
CALIBRATED_WEIGHTS = {
    'gravity': 1.0,      # Equal weight - gravity data available
    'magnetic': 1.0,     # Equal weight - magnetic data available
    'lithology': 1.0,    # Equal weight - lithology data available
    'insar': 0.0,        # Zero weight - InSAR data not available
    'dem_slope': 0.5,    # Half weight - DEM data partially available
}

CALIBRATED_WEIGHTS_FILE = OUTPUT_DIR / "calibrated_weights.json"

def load_calibrated_weights() -> Dict[str, float]:
    """Load calibrated layer weights from disk or return defaults.
    
    Attempts to load scientifically-calibrated weights from a JSON file
    generated by the calibration script. Falls back to default weights
    if the file doesn't exist or can't be loaded.
    
    Returns:
        Dict[str, float]: Dictionary mapping layer names to weights.
            Keys: 'gravity', 'magnetic', 'insar', 'lithology', 'seismic', etc.
            Values: Positive float weights (typically 0.0 to 1.0)
    
    Notes:
        - Weights are normalized internally by calculate_void_probability
        - Zero weights exclude a layer from probability calculation
        - File path: CALIBRATED_WEIGHTS_FILE (outputs/void_detection/calibrated_weights.json)
    """
    try:
        if CALIBRATED_WEIGHTS_FILE.exists():
            import json
            data = json.load(open(CALIBRATED_WEIGHTS_FILE, 'r'))
            # Keep only positive weights and known keys
            weights = {k: float(v) for k, v in data.items() if v is not None}
            return weights
    except Exception as e:
        logger.warning(f"Failed to load calibrated weights: {e}")
    return DEFAULT_WEIGHTS.copy()


def _lonlat_to_rowcol(bounds: Tuple[float, float, float, float], resolution: float, lon: float, lat: float) -> Tuple[int, int]:
    """Convert geographic coordinates to raster row/column indices.
    
    Args:
        bounds: Geographic bounds (lon_min, lat_min, lon_max, lat_max) in degrees
        resolution: Grid resolution in degrees per pixel
        lon: Longitude of point in degrees
        lat: Latitude of point in degrees
    
    Returns:
        Tuple[int, int]: (row, col) indices clamped to valid raster bounds
    
    Notes:
        - Row 0 is at lat_max (standard raster orientation)
        - Column 0 is at lon_min
        - Indices are clamped to [0, width-1] and [0, height-1]
        - Assumes WGS84 geographic coordinate system
    """
    lon_min, lat_min, lon_max, lat_max = bounds
    width = int((lon_max - lon_min) / resolution)
    height = int((lat_max - lat_min) / resolution)
    col = int((lon - lon_min) / resolution)
    row = int((lat_max - lat) / resolution)
    col = max(0, min(width - 1, col))
    row = max(0, min(height - 1, row))
    return row, col


def calibrate_and_save_weights(
    bounds: Tuple[float, float, float, float],
    resolution: float,
    score_layers: Dict[str, Optional[np.ndarray]],
) -> Dict[str, float]:
    """Calibrate per-layer weights using known features via logistic regression.

    Saves weights to CALIBRATED_WEIGHTS_FILE and returns the dict.
    """
    try:
        from sklearn.linear_model import LogisticRegression
        import json
        # Lazy import to avoid hard dependency at import time
        import validate_against_known_features as val
    except Exception as e:
        logger.error(f"Calibration requires scikit-learn and validation module: {e}")
        return load_calibrated_weights()

    # Build dataset
    X = []
    y = []
    feature_names = [k for k, v in score_layers.items() if v is not None]
    if not feature_names:
        logger.error("No score layers available for calibration")
        return load_calibrated_weights()

    for feat in val.KNOWN_FEATURES:
        lon = float(feat['lon']); lat = float(feat['lat'])
        # Filter to features inside bounds
        lon_min, lat_min, lon_max, lat_max = bounds
        if not (lon_min <= lon <= lon_max and lat_min <= lat <= lat_max):
            continue
        row, col = _lonlat_to_rowcol(bounds, resolution, lon, lat)
        # 3x3 window median
        sample = []
        for name in feature_names:
            arr = score_layers[name]
            if arr is None:
                sample.append(0.0)
                continue
            r0 = max(0, row - 1); r1 = min(arr.shape[0], row + 2)
            c0 = max(0, col - 1); c1 = min(arr.shape[1], col + 2)
            window = arr[r0:r1, c0:c1]
            valm = float(np.nanmedian(window)) if window.size > 0 else 0.0
            sample.append(valm)
        X.append(sample)
        # Label: 1 for expected negative (void-like), 0 otherwise
        expected = str(feat.get('expected', 'negative')).lower()
        y.append(1 if expected == 'negative' else 0)

    if len(X) < 3:
        logger.warning("Insufficient feature samples for calibration; using defaults")
        return load_calibrated_weights()

    X = np.asarray(X, dtype=np.float32)
    y = np.asarray(y, dtype=np.int32)

    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)
    coef = model.coef_[0]
    # Clamp to non-negative contributions and normalize
    raw_weights = {name: max(float(w), 0.0) for name, w in zip(feature_names, coef)}
    s = sum(raw_weights.values())
    if s > 0:
        weights = {k: v / s for k, v in raw_weights.items()}
    else:
        weights = load_calibrated_weights()

    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        with open(CALIBRATED_WEIGHTS_FILE, 'w') as f:
            json.dump(weights, f, indent=2)
        logger.info(f"Saved calibrated weights: {CALIBRATED_WEIGHTS_FILE}")
    except Exception as e:
        logger.warning(f"Failed to save calibrated weights: {e}")

    return weights


# ============================================================================
# CORE FUNCTIONS
# ============================================================================

def load_and_resample(
    src_path: Path,
    target_bounds: Tuple[float, float, float, float],
    target_res: float,
    target_crs: str = "EPSG:4326"
) -> np.ndarray:
    """Load and resample raster to target grid."""
    if not src_path.exists():
        logger.warning(f"File not found: {src_path}")
        return None
    
    minx, miny, maxx, maxy = target_bounds
    width = int((maxx - minx) / target_res)
    height = int((maxy - miny) / target_res)
    transform = from_bounds(minx, miny, maxx, maxy, width, height)
    
    with rasterio.open(src_path) as src:
        # Create destination array
        dst_array = np.zeros((height, width), dtype=np.float32)
        
        # Reproject to target grid
        reproject(
            source=rasterio.band(src, 1),
            destination=dst_array,
            src_transform=src.transform,
            src_crs=src.crs,
            dst_transform=transform,
            dst_crs=target_crs,
            resampling=Resampling.bilinear,
            src_nodata=src.nodata,
            dst_nodata=np.nan
        )
    
    return dst_array


def score_gravity(data: np.ndarray) -> np.ndarray:
    """
    Score gravity anomalies for void probability.
    
    Negative anomalies indicate mass deficits (potential voids).
    Score: 0 (no void signature) to 1 (strong void signature)
    """
    if data is None:
        return None
    
    # Handle NaN values before processing to prevent propagation
    nan_mask = np.isnan(data)
    data_clean = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Normalize: more negative = higher probability
    # Clamp between -6σ and 0σ
    score = np.clip(-data_clean / 6.0, 0, 1)
    
    # Restore NaN mask to preserve data validity information
    score = np.where(nan_mask, np.nan, score)
    
    return score.astype(np.float32)


def score_insar(data: np.ndarray) -> np.ndarray:
    """
    Score InSAR subsidence for void probability.
    
    Subsidence (negative velocity) indicates ground settling over voids.
    Score: 0 (no subsidence) to 1 (strong subsidence)
    """
    if data is None:
        return None
    
    # Handle NaN values before processing to prevent propagation
    nan_mask = np.isnan(data)
    data_clean = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Normalize: more negative = higher probability
    # -50 mm/year = maximum score
    score = np.clip(-data_clean / 50.0, 0, 1)
    
    # Restore NaN mask to preserve data validity information
    score = np.where(nan_mask, np.nan, score)
    
    return score.astype(np.float32)


def score_lithology(data: np.ndarray, lithology_codes: Dict) -> np.ndarray:
    """
    Score lithology for karst susceptibility.
    
    Limestone, dolomite, evaporites = high void probability
    Score: 0 (non-karst) to 1 (highly susceptible)
    """
    if data is None:
        return None
    
    # Create binary mask for karst-prone rocks
    karst_mask = np.isin(data, list(lithology_codes.values()))
    score = karst_mask.astype(np.float32)
    
    return score


def score_seismic(data: np.ndarray) -> np.ndarray:
    """
    Score seismic velocity for void probability.
    
    Low velocity zones indicate fractured/void-rich rock.
    Score: 0 (normal velocity) to 1 (very low velocity)
    """
    if data is None:
        return None
    
    # Handle NaN values before processing to prevent propagation
    nan_mask = np.isnan(data)
    data_clean = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Normalize: lower velocity = higher probability
    # 3 km/s = maximum score (very low for crustal rock)
    score = np.clip((5.0 - data_clean) / 2.0, 0, 1)
    
    # Restore NaN mask to preserve data validity information
    score = np.where(nan_mask, np.nan, score)
    
    return score.astype(np.float32)


def calculate_void_probability(
    layer_scores: Dict[str, Optional[np.ndarray]],
    layer_weights: Optional[Dict[str, float]] = None,
) -> Optional[np.ndarray]:
    """Calculate overall void probability from multiple weighted data layers.
    
    Fuses scores from multiple geophysical layers (gravity, magnetic, InSAR, lithology)
    using weighted averaging to produce a single probability map. Missing data (NaN)
    propagates to the output.
    
    Args:
        layer_scores: Dictionary mapping layer names to scored arrays (0-1 range).
            Keys: 'gravity', 'magnetic', 'insar', 'lithology', 'seismic', etc.
            Values: np.ndarray with shape (height, width) or None if unavailable.
            Scores should be normalized to [0, 1] where 1 = highest void probability.
        layer_weights: Optional dictionary of weights for each layer.
            If None, loads calibrated weights from disk or uses defaults.
            Weights are automatically normalized to sum to 1.0.
    
    Returns:
        np.ndarray: Probability map with shape matching input layers.
            Values range from 0.0 (no void) to 1.0 (very likely void).
            Returns np.nan where any input layer has missing data.
            Returns zeros if no valid layers available.
    
    Raises:
        ValueError: If no valid data layers available and shape cannot be determined.
    
    Notes:
        - All input arrays must have the same shape and georeferencing
        - Weights are normalized internally (don't need to sum to 1.0)
        - Layers with zero or negative weights are excluded
        - Missing data (NaN) in any layer propagates to output
        - At least one non-zero weighted layer must be available
    
    Example:
        >>> scores = {
        ...     'gravity': gravity_score_array,  # shape (1000, 1000)
        ...     'magnetic': magnetic_score_array,
        ...     'lithology': None  # not available
        ... }
        >>> weights = {'gravity': 0.6, 'magnetic': 0.4, 'lithology': 0.0}
        >>> prob = calculate_void_probability(scores, weights)
        >>> prob.shape  # (1000, 1000)
    """
    # Collect available scores
    if layer_weights is None:
        layer_weights = load_calibrated_weights()

    scores: list[np.ndarray] = []
    weights: list[float] = []
    for name, arr in layer_scores.items():
        if arr is None:
            continue
        w = layer_weights.get(name, 0.0)
        if w <= 0:
            continue
        scores.append(arr)
        weights.append(w)

    if not scores:
        logger.warning("No valid scores available for void probability calculation")
        # Return zero-filled array instead of None to prevent downstream crashes
        # Use the shape from layer_scores if available, otherwise create minimal array
        reference_shape = None
        for arr in layer_scores.values():
            if arr is not None:
                reference_shape = arr.shape
                break
        
        if reference_shape is None:
            # Fallback to minimal 1x1 array if no reference available
            logger.error("No data layers available for void detection - cannot determine output shape!")
            raise ValueError("No valid data layers available for void probability calculation")
        
        return np.zeros(reference_shape, dtype=np.float32)
    
    # Normalize weights
    weights = np.array(weights)
    weights = weights / weights.sum()
    
    # Calculate weighted average
    probability = np.zeros_like(scores[0], dtype=np.float32)
    for score, weight in zip(scores, weights):
        probability += score.astype(np.float32) * float(weight)

    return probability.astype(np.float32)


def identify_hotspots(
    probability: np.ndarray,
    threshold: float = 0.7,
    min_cluster_size: int = 5
) -> np.ndarray:
    """
    Identify high-probability void clusters.
    
    Returns labeled array of connected components above threshold.
    """
    from scipy import ndimage
    
    # Binary mask of high-probability pixels
    mask = probability >= threshold
    
    # Label connected components
    labeled, num_features = ndimage.label(mask)
    
    # Filter small clusters
    for label_id in range(1, num_features + 1):
        cluster_size = np.sum(labeled == label_id)
        if cluster_size < min_cluster_size:
            labeled[labeled == label_id] = 0
    
    return labeled


def visualize_void_probability(
    probability: np.ndarray,
    output_path: Path,
    bounds: Tuple[float, float, float, float],
    hotspots: Optional[np.ndarray] = None
):
    """Create publication-quality void probability map."""
    
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # Custom colormap: white (low) -> yellow -> orange -> red (high)
    colors = ['white', 'yellow', 'orange', 'red', 'darkred']
    n_bins = 100
    cmap = mcolors.LinearSegmentedColormap.from_list('void_prob', colors, N=n_bins)
    
    # Plot probability
    im = ax.imshow(
        probability,
        cmap=cmap,
        extent=[bounds[0], bounds[2], bounds[1], bounds[3]],
        vmin=0,
        vmax=1,
        origin='upper',
        interpolation='bilinear'
    )
    
    # Overlay hotspot boundaries
    if hotspots is not None and np.any(hotspots):
        y_coords = np.linspace(bounds[1], bounds[3], hotspots.shape[0])
        x_coords = np.linspace(bounds[0], bounds[2], hotspots.shape[1])
        ax.contour(
            x_coords,
            y_coords,
            hotspots,
            levels=[0.5],
            colors='blue',
            linewidths=2,
        )
    
    # Formatting
    ax.set_xlabel('Longitude (°)', fontsize=12)
    ax.set_ylabel('Latitude (°)', fontsize=12)
    ax.set_title('Underground Void Probability Map\n(Higher values = greater likelihood)', 
                 fontsize=14, fontweight='bold')
    
    # Colorbar
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('Void Probability', fontsize=12)
    
    # Grid
    ax.grid(True, alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"Saved visualization: {output_path}")
    plt.close()


def write_geotiff(
    data: np.ndarray,
    output_path: Path,
    bounds: Tuple[float, float, float, float],
    crs: str = "EPSG:4326"
):
    """Write probability map as GeoTIFF."""
    
    height, width = data.shape
    transform = from_bounds(bounds[0], bounds[1], bounds[2], bounds[3], width, height)
    
    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=height,
        width=width,
        count=1,
        dtype=data.dtype,
        crs=crs,
        transform=transform,
        nodata=np.nan,
        compress='DEFLATE',
        predictor=2,
        tiled=True,
        blockxsize=512,
        blockysize=512
    ) as dst:
        dst.write(data, 1)
        dst.set_band_description(1, "Void Probability (0-1)")
    
    logger.info(f"Saved GeoTIFF: {output_path}")


# ============================================================================
# MAIN PROCESSING
# ============================================================================

def process_region(
    bounds: Tuple[float, float, float, float],
    resolution: float = 0.001,  # ~100m at equator
    output_base: str = "void_probability",
    prob_threshold: float = 0.7,
    calibrate: bool = False,
):
    """
    Process region and generate void probability map.
    
    Args:
        bounds: (lon_min, lat_min, lon_max, lat_max)
        resolution: Grid resolution in degrees
        output_base: Output filename base
    """
    
    logger.info(f"Processing region: {bounds}")
    logger.info(f"Resolution: {resolution}° (~{resolution * 111:.0f} km)")
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Load and resample data layers
    logger.info("Loading gravity data...")
    gravity_data = load_and_resample(GRAVITY_PATH, bounds, resolution)
    
    logger.info("Loading magnetic data (if available)...")
    magnetic_data = load_and_resample(MAGNETIC_PATH, bounds, resolution)
    
    logger.info("Loading InSAR data (if available)...")
    insar_data = load_and_resample(INSAR_PATH, bounds, resolution)
    
    logger.info("Loading lithology data (if available)...")
    lithology_data = None
    lithology_codes: Dict = {}
    try:
        if LITHOLOGY_PATH.exists() and LITHOLOGY_PATH.is_dir():
            # Prefer a GeoTIFF if present
            tifs = sorted(LITHOLOGY_PATH.glob("*.tif"))
            if tifs:
                lithology_data = load_and_resample(tifs[0], bounds, resolution)
            # Optional codebook
            codebook = LITHOLOGY_PATH / "codes.json"
            if codebook.exists():
                import json
                lithology_codes = json.load(open(codebook, 'r'))
    except Exception as e:
        logger.warning(f"Failed to load lithology data: {e}")
    
    logger.info("Loading seismic data (if available)...")
    seismic_data = None  # Seismic data not yet implemented
    
    # Score each layer
    logger.info("Calculating void probability scores...")
    gravity_score = score_gravity(gravity_data)
    magnetic_score = score_gravity(magnetic_data) if magnetic_data is not None else None  # Magnetic anomalies scored like gravity
    insar_score = score_insar(insar_data)
    lithology_score = score_lithology(lithology_data, lithology_codes)
    seismic_score = score_seismic(seismic_data)

    score_dict = {
        'gravity': gravity_score,
        'magnetic': magnetic_score,
        'insar': insar_score,
        'lithology': lithology_score,
        'seismic': seismic_score,
    }
    if calibrate:
        logger.info("Calibrating layer weights against known features...")
        calibrate_and_save_weights(bounds, resolution, score_dict)
    # Calculate combined probability with independent layer weights
    probability = calculate_void_probability(score_dict)
    
    if probability is None:
        logger.error("Failed to calculate void probability")
        return
    
    # Identify hotspots
    logger.info("Identifying high-probability void zones...")
    hotspots = identify_hotspots(probability, threshold=prob_threshold)
    num_hotspots = hotspots.max()
    logger.info(f"Found {num_hotspots} potential void clusters")
    
    # Save outputs
    output_tif = OUTPUT_DIR / f"{output_base}.tif"
    output_png = OUTPUT_DIR / f"{output_base}.png"
    
    write_geotiff(probability, output_tif, bounds)
    visualize_void_probability(probability, output_png, bounds, hotspots)
    
    # Generate report
    report_path = OUTPUT_DIR / f"{output_base}_report.txt"
    with open(report_path, 'w') as f:
        f.write("VOID DETECTION REPORT\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Region: {bounds}\n")
        f.write(f"Resolution: {resolution}° (~{resolution * 111:.0f} km)\n\n")
        f.write(f"Data Layers Used:\n")
        f.write(f"  - Gravity: {'YES' if gravity_data is not None else 'NO'}\n")
        f.write(f"  - Magnetic: {'YES' if magnetic_data is not None else 'NO'}\n")
        f.write(f"  - InSAR: {'YES' if insar_data is not None else 'NO'}\n")
        f.write(f"  - Lithology: {'YES' if lithology_data is not None else 'NO'}\n")
        f.write(f"  - Seismic: {'YES' if seismic_data is not None else 'NO'}\n\n")
        f.write(f"Results:\n")
        f.write(f"  - High-probability zones (>{prob_threshold}): {num_hotspots}\n")
        f.write(f"  - Mean probability: {np.nanmean(probability):.3f}\n")
        f.write(f"  - Max probability: {np.nanmax(probability):.3f}\n\n")
        f.write(f"Outputs:\n")
        f.write(f"  - Probability map: {output_tif}\n")
        f.write(f"  - Visualization: {output_png}\n")
    
    logger.info(f"Report saved: {report_path}")
    logger.info("Void detection complete!")
    return output_tif


def main():
    parser = argparse.ArgumentParser(
        description="Detect underground voids using multi-layer geophysical analysis"
    )
    parser.add_argument(
        '--region',
        type=str,
        help='Region bounds: "lon_min,lat_min,lon_max,lat_max" (default: Carlsbad Caverns area)',
        default="-105.0,32.0,-104.0,33.0"
    )
    parser.add_argument(
        '--resolution',
        type=float,
        default=0.001,
        help='Grid resolution in degrees (default: 0.001° ~ 100m)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='void_probability',
        help='Output filename base'
    )
    parser.add_argument(
        '--prob-threshold',
        type=float,
        default=0.7,
        help='Probability threshold for hotspot identification (default: 0.7)'
    )
    parser.add_argument(
        '--calibrate',
        action='store_true',
        help='Fit and persist layer weights using known features in-bounds'
    )
    
    args = parser.parse_args()
    
    # Parse bounds
    bounds_str = args.region.split(',')
    if len(bounds_str) != 4:
        logger.error("Invalid region format. Use: lon_min,lat_min,lon_max,lat_max")
        sys.exit(1)
    
    bounds = tuple(map(float, bounds_str))
    
    # Process
    process_region(bounds, args.resolution, args.output, prob_threshold=args.prob_threshold, calibrate=args.calibrate)


if __name__ == "__main__":
    main()
